



VGG使用可重复使用的卷积块来构建深度卷积神经网络
不同的卷积快个数和超参数可以得到不同复杂度的变种
  有一个经典的思想：长和宽减半，通道数翻倍


卷积层的输入和输出由四维张量组成，张量的每个轴分别对应样本、通道、高度和宽度

NiN块：
  全连接层的问题：参数可能过多
  全连接层不是很好，用卷积层来替代掉

  整体架构：
  1.没有全连接层
  2.交替使用NiN块和步幅为2的最大池化层
    逐步减少高宽和增大通道数
  3.最后使用全局平均池化层的到输出
    其输入通道数是类别数

  总结：
  1.NiN块使用卷积层加两个1x1卷积层
    后者对每个像素增加了非线性性
  2.NiN使用全局平均池化层来替代VGG和AlexNet中的全连接层
    不容易过拟合，更少的参数个数



GoogleNet:
  inception块：不仅增加了多样性(大量的不同的卷积层，而且参数变少了)



批量归一化：
  出现的问题：
    损失出现在最后，后面的层训练较快
    数据出现在最底部：
        底部的层训练较慢
        底部层一变化，所有都得跟着变
        最后的那些层需要重新学习多次
        导致收敛变慢
  让我们来回顾一下训练神经网络时出现的一些实际挑战。

首先，数据预处理的方式通常会对最终结果产生巨大影响。 回想一下我们应用多层感知机来预测房价的例子（ 4.10节）。 使用真实数据时，我们的第一步是标准化输入特征，使其平均值为0，方差为1。 直观地说，这种标准化可以很好地与我们的优化器配合使用，因为它可以将参数的量级进行统一。

第二，对于典型的多层感知机或卷积神经网络。当我们训练时，中间层中的变量（例如，多层感知机中的仿射变换输出）可能具有更广的变化范围：不论是沿着从输入到输出的层，跨同一层中的单元，或是随着时间的推移，模型参数的随着训练更新变幻莫测。 批量规范化的发明者非正式地假设，这些变量分布中的这种偏移可能会阻碍网络的收敛。 直观地说，我们可能会猜想，如果一个层的可变值是另一层的100倍，这可能需要对学习率进行补偿调整。

第三，更深层的网络很复杂，容易过拟合。 这意味着正则化变得更加重要。



  核心想法：
    方差和均值的分布在不同层之间会变化；
    把分布固定住，不管每一层的输出和梯度都符合某一个分布，相对来说就比较稳定，这会使得在学习中间细微的东西时会比较容易

  批量归一化固定小批量中的均值和方差，然后学习出适合的偏移和缩放
  可以加速收敛速度，但一般不改变模型精度

  批量归一化层：
  可学习的参数为ρ和β
  作用在：
      全连接层和卷积层的输出上，激活函数前
      全连接层和卷积层的输入上
  对全连接层，作用在特征维
  对卷积层，作用在通道维

  批量归一化层的moving_mean和moving_var在训练中可以更新，来确保在推理时用moving_mean和moving_var，避免由于batch_size=1造成的均值和方差不稳定的情况


