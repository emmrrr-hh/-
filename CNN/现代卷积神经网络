



VGG使用可重复使用的卷积块来构建深度卷积神经网络
不同的卷积快个数和超参数可以得到不同复杂度的变种
  有一个经典的思想：长和宽减半，通道数翻倍


卷积层的输入和输出由四维张量组成，张量的每个轴分别对应样本、通道、高度和宽度

NiN块：
  全连接层的问题：参数可能过多
  全连接层不是很好，用卷积层来替代掉

  整体架构：
  1.没有全连接层
  2.交替使用NiN块和步幅为2的最大池化层
    逐步减少高宽和增大通道数
  3.最后使用全局平均池化层的到输出
    其输入通道数是类别数

  总结：
  1.NiN块使用卷积层加两个1x1卷积层
    后者对每个像素增加了非线性性
  2.NiN使用全局平均池化层来替代VGG和AlexNet中的全连接层
    不容易过拟合，更少的参数个数



GoogleNet:
  inception块：不仅增加了多样性(大量的不同的卷积层，而且参数变少了)



批量归一化：
  批量归一化固定小批量中的均值和方差，然后学习出适合的偏移和缩放
  可以加速收敛速度，但一般不改变模型精度

  批量归一化层：
  可学习的参数为ρ和β
  作用在：
      全连接层和卷积层的输出上，激活函数前
      全连接层和卷积层的输入上
  对全连接层，作用在特征维
  对卷积层，作用在通道维

  批量归一化层的moving_mean和moving_var在训练中可以更新，来确保在推理时用moving_mean和moving_var，避免由于batch_size=1造成的均值和方差不稳定的情况


