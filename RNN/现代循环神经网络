门控循环单元(GRU)
  不是每个观察值都是同等重要
  想只记住相关的观察需要：
    能关注的机制(更新门)
    能遗忘的机制(重置门)

  Ht分为三种极端情况：
    1.只和Ht-1相关--->Zt为接近于1
    2.只和Xt相关--->Rt接近于0，Zt接近于0
    3.和原来的RNN一样--->Rt接近于1，Zt接近于1




长短期记忆网络(LSTM):
  有两个状态，一个C(记忆单元)，一个H 
  忘记门：将值朝0减少
  输入门：决定是不是忽略掉输入数据
  输出门：决定是不是使用隐状态

  候选记忆单元：和RNN的Ht更新算法一样

  H要保证[-1,1],C用来储存信息，可以做到数值比较大

深度循环神经网络：
  如何得到更多的非线性性：


  每个隐藏状态由左边和下边的状态实现


双向循环神经网络：
  通过反向更新的隐藏层来利用方向时间信息
  取决于过去和未来的上下文(比如运用于完型填空)

  有两个隐藏层，两个隐藏层的方向不一样

  双向循环神经网络：不适合作推理，因为双向循环网络在看不到后面的情况时是无法运行的
  通常用来对序列抽取特征，填空，而不能预测未来


编码器和解码器架构：
  


Seq2seq：
  编码器是一个RNN，读取输入句子--->没有输出的RNN
    可以是双向
  解码器可以使用另外一个RNN来输出

  从一个句子生成另一个句子
  编码器、解码器都是RNN
  将编码器的最后时间隐状态来初始化解码器隐状态来完成信息传递
  常用BLEU来衡量生成序列的好坏


衡量指标：BLEU：
    BLEU越大越好
