微调，迁移学习的一个技巧

背景引入：标注一个数据集很贵

一个神经网络一般可以分为两类：
  特征抽取将原始像素变成容易线性分割的特征
  线性分类器来做分类

微调：
  不能直接使用原来的线性分类器，因为标号可能变了
  可能仍然对我数据集做特征抽取

训练：
  是一个目标数据集上的正常训练任务，但使用更强的正则化
    使用更小的学习率
    使用更少的数据迭代
  源数据集远复杂于目标数据集，通常微调效果更好

  重用分类器权重
    源数据集可能也有目标数据中的部分标号
    可以使用预训练好模型分类器中对应标号对应的向量来做初始化（参考softmax回归的相关知识）

  固定一些层：
  神经网络通常学习有层次的特征表示
    低层次的特征更加通用
    高层次的他特征则更跟数据集有关
  可以固定底部一些层的参数，不参与更新
    更强的正则(因为固定住了一些底部的层不参与更更新，因此相当于更强的正则化)



过程：
  1.在源数据集（例如ImageNet数据集）上预训练神经网络模型，即源模型。
  
  2.创建一个新的神经网络模型，即目标模型。这将复制源模型上的所有模型设计及其参数（输出层除外）。我们假定这些模型参数包含从源数据集中学到的知识，这些知识也将适用于目标数据集。我们还假设源模型的输出层与源数据集的标签密切相关；因此不在目标模型中使用该层。
  
  3.向目标模型添加输出层，其输出数是目标数据集中的类别数。然后随机初始化该层的模型参数。
  
  4.在目标数据集（如椅子数据集）上训练目标模型。输出层将从头开始进行训练，而所有其他层的参数将根据源模型的参数进行微调。


总结：
微调通过使用在大数据得到的预训练好的模型来初始化模型权重来提升精度
预训练模型质量很重要
微调通常速度更快、精度更高


