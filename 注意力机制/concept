心理学：
动物需要在复杂的环境下有效关注值得注意的点
心理学框架：人类根据随意线索和不随意线索选择注意点


注意力机制：
  卷积、全连接、池化层都只考虑不随意线索
  注意力机制则显示的考虑随意线索：
    随意线索被成为查询(query)
    每个输入是一个值(value)和不随意线索(key)的对
    通过注意力池化层来有偏向性的选择某些输入
    
  首先会有很多key，每个key都有对应的value
  输入一个query，寻找query和key之间的关系，得出query对应的一个值，
  下面的非参数注意力机制和带参数注意力机制说明了如何操作：
  非参数注意力汇聚：query和key之间的关系通过一个K函数然后在query和所有key的关系之间做softmax来生成权重，对于所有的values进行加权得到query对应的value
  参数注意力汇聚：，在非参数注意力汇聚的基础上，K函数的运算过程中添加一个名为ω的变量
  

非参数注意力汇聚：
  好处：不需要学；
  但是需要足够多的数据

带参数注意力汇聚缩小了窗口，使得query和其相近的key的值对应的value所占的权重比较大

注意力分数是query和key的相似度，注意力权重是分数的softmax结果
两种常见的分数计算：
  将query和key合并起来进入一个单输出单隐藏层的MLP
  直接将query和key作内积






Seq2seq通过隐状态在编码器和解码器中传递信息
注意力机制可以根据解码器RNN的输出来匹配到合适的编码器RNN的输出来更有效的传递信息


自注意力：
  query，key和value如何选

  适合处理比较长的文本

  位置编码：
    跟CNN/RNN不同，自注意力并没有记录位置信息
    位置编码将位置信息注入到输入里
      （如果位置信息要放到模型中的话会带来一系列问题，可能造成并行度下降等） 
        
      

BERT:只有编码器的Transformer


