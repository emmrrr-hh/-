心理学：
动物需要在复杂的环境下有效关注值得注意的点
心理学框架：人类根据随意线索和不随意线索选择注意点


注意力机制：
  卷积、全连接、池化层都只考虑不随意线索
  注意力机制则显示的考虑随意线索：
    随意线索被成为查询(query)
    每个输入是一个值(value)和不随意线索(key)的对
    通过注意力池化层来有偏向性的选择某些输入
    
  首先会有很多key，每个key都有对应的value
  输入一个query，寻找query和key之间的关系，得出query对应的一个值，
  下面的非参数注意力机制和带参数注意力机制说明了如何操作：
  非参数注意力汇聚：query和key之间的关系通过一个K函数然后在query和所有key的关系之间做softmax来生成权重，对于所有的values进行加权得到query对应的value
  参数注意力汇聚：，在非参数注意力汇聚的基础上，K函数的运算过程中添加一个名为ω的变量
  

非参数注意力汇聚：
  好处：不需要学；
  但是需要足够多的数据

带参数注意力汇聚缩小了窗口，使得query和其相近的key的值对应的value所占的权重比较大

注意力分数是query和key的相似度，注意力权重是分数的softmax结果
两种常见的分数计算：
  将query和key合并起来进入一个单输出单隐藏层的MLP
  直接将query和key作内积






Seq2seq通过隐状态在编码器和解码器中传递信息
注意力机制可以根据解码器RNN的输出来匹配到合适的编码器RNN的输出来更有效的传递信息


自注意力：
  解释： 想象一下，有了注意力机制之后，我们将词元序列输入注意力池化中， 以便同一组词元同时充当查询、键和值。 具体来说，每个查询都会关注所有的键－值对并生成一个注意力输出。 由于查询、键和值来自同一组输入，因此被称为 自注意力（self-attention）
  query，key和value如何选
  
  适合处理比较长的文本

  位置编码：
    跟CNN/RNN不同，自注意力并没有记录位置信息
    位置编码将位置信息注入到输入里
      （如果位置信息要放到模型中的话会带来一系列问题，可能造成并行度下降等） 
        


Transformer架构:
  基于编码器-解码器架构来处理序列对
  跟使用注意力的seq2seq不同，transformer是纯基于注意力



  基于位置的前馈神经网络：
    将输入形状由(b,n,d)变为(bn,d)
    作用两个全连接层
    输出形状由(bn,d)变化回(b,n,d)
    等价于两层核窗口为1的一维卷积层

BERT:只有编码器的Transformer


