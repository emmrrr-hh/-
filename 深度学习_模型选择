训练数据集：训练模型参数
验证数据集：训练模型超参数
非大数据集熵通常使用k-折交叉验证


过拟合与欠拟合

解决过拟合的方案：
  权重衰退：修改损失函数，在损失函数上加上一个L2正则化项；正则化项的权重是控制模型复杂度的超参数
  目的--->降低模型的复杂度



  暂退法（dropout）:神经网络中每个节点的值有概率为0，有一个tip可以使得E|h'|=E|h|
    丢弃概率是一个超参数
  动机：一个好的模型需要对输入数据的扰动鲁棒
      使用有噪音的数据等价于Tikhonov正则
     暂退法：在层之间加入噪音

  正则项只在训练中使用，推理过程中并不需要加入正则化项



数值稳定性：
  梯度爆炸：
    带来问题：
    1.值超出值域
    2.对学习率敏感：
      学习率太大-->大参数值-->更大梯度
      学习率太小-->小参数值-->训练无进展
      解决：我们可能在训练过程中需要不断调整学习率

  梯度消失：
    带来的问题：
    1.梯度值变成0：对16位浮点数尤为严重
    2.训练没有进展
      不管如何选择学习率
    3.对于底部层尤为严重
      仅仅顶部层训练的较好
      无法让神经网络更深
  总结:当数值过大或者过小时会导致数值问题；常发生在深度模型中，因为其会对n个数累乘


让训练更加稳定：
  目标：让梯度值在合理的范围内
  做法： 
    将乘法变成加法；ResNet，LSTM
    归一化：梯度归一化，梯度裁剪
    合理的权重初始和激活函数


  让每一层的梯度都是一个常数



  权重初始化：
    Xavier初始化


  激活函数：
    假设线性的激活函数：激活函数σ(x)=αx+β，β=0，α=1，这样的激活函数可以保证正向输出和反向梯度的均值为0，方差为固定值。
  
  
