
一、向量和矩阵的区别？
  向量1维，矩阵多维
二、线性代数的实现：
  torch.arange
  torch.shape()
  torch.reshape()
  X.sum()
    1.  一般输出一个标量
    2。按特定轴求和
      a. X.sum(axis=0，1或者[0,1])     b. X.sum(axis=0,keepsdims)
      举例：
      [[1,2,3,4]
       [5,6,7,8]]
      axis=0:
          shape[5,4]--->shape[4]
          沿着行---->[1+2+3+4,5+6+7+8]
      axis=1:
          shape[5,4]--->shape[5]
          沿着列---->[1+5,2+6,3+7,4+8]
      参数keepdims=True的使用，不删除指定的维度而是把指定的维度置为1
      举例：
      axis=0,keepdims=True
        shape[5,4]--->shape[1,4]
  向量的点积：
    X*Y
  torch.dot():
    按元素点积后求和
  torch.mv(A,x):
    矩阵乘向量
      A为(m,n),x为(n,)//A为m*n的矩阵，B为n维向量
  torch.mm(A,B)
    两个矩阵做乘法
  torch.norm(u)
    求向量u的L2范数(L2范数：每个元素平方和的开方)
    L1范数：向量元素的绝对值之和
三、矩阵计算
  导数
  亚导数：比如|x|的0处的导数为a，a∈[-1,1]；max(x,0)的0处的导数为a，a∈[0,1]
  将导数拓展到向量：
  y是标量，x是向量,x=[x1,x2,x3,...,xn],
  导数：dy/dx=[dy/dx1,dy/dx2,dy/dx3,...dy/dxn]
  梯度：指向一个值变化最大的那个方向（why?设Δf为梯度，v为方向向量，当v与Δf方向一致时，得到的Δf*v的值是最大的）（机器学习求解的一个核心思想）
  

