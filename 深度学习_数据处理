
一、向量和矩阵的区别？
  向量1维，矩阵多维
二、线性代数的实现：
  torch.arange
  torch.shape()
  torch.reshape()
  X.sum()
    1.  一般输出一个标量
    2。按特定轴求和
      a. X.sum(axis=0，1或者[0,1])     b. X.sum(axis=0,keepsdims)
      举例：
      [[1,2,3,4]
       [5,6,7,8]]
      axis=0:
          shape[5,4]--->shape[4]
          沿着行---->[1+2+3+4,5+6+7+8]
      axis=1:
          shape[5,4]--->shape[5]
          沿着列---->[1+5,2+6,3+7,4+8]
      参数keepdims=True的使用，不删除指定的维度而是把指定的维度置为1
      举例：
      axis=0,keepdims=True
        shape[5,4]--->shape[1,4]
  向量的点积：
    X*Y
  torch.dot():
    按元素点积后求和
  torch.mv(A,x):
    矩阵乘向量
      A为(m,n),x为(n,)//A为m*n的矩阵，B为n维向量
  torch.mm(A,B)
    两个矩阵做乘法
  torch.norm(u)
    求向量u的L2范数(L2范数：每个元素平方和的开方)
    L1范数：向量元素的绝对值之和
三、矩阵计算
  导数
  亚导数：比如|x|的0处的导数为a，a∈[-1,1]；max(x,0)的0处的导数为a，a∈[0,1]
  将导数拓展到向量：
  y是标量，x是向量,x=[x1,x2,x3,...,xn],
  导数：dy/dx=[dy/dx1,dy/dx2,dy/dx3,...dy/dxn]
  梯度：指向一个值变化最大的那个方向（why?设Δf为梯度，v为方向向量，当v与Δf方向一致时，得到的Δf*v的值是最大的）（机器学习求解的一个核心思想）

  y为标量，x,u,v为向量,
    y=a:dy/dx=0.T
    y=au(u为向量)：dy/dx=adu/dx
    y=sum(x)：1.T
    ||x||^2:2x.T
    y=u+v:dy/dx=du/dx+dv/dx
    y=uv:vdu/dx+udv/dx
    y=<u,v>:dy/dx=u.Tdv/dx+v.Tdu/dx

  y是列向量，x为标量：
  dy/dx是一个列向量

  y是一个向量，x为向量：
  dy/dx为一个矩阵
    y=a:dy/dx=0;
    y=x:dy/dx=I;
    y=Ax:dy/dx=A;
    y=x.T*A;dy/dx=A.T;
四、向量链式法则：
  标量链式法则：
  向量链式法则：
  y=f(u),u=g(x)
  y为标量，x为n维向量，u为标量：dy/dx=dy/du*du/dx--->(1,n)<--(1,)(1,n)
  y为标量，x为n维向量，u为k维向量：dy/dx=dy/du*du/dx---->(1,n)<--(1,k)(k,n)
  y为向量，x为n维向量，u为k维向量：dy/dx=dy/du*du/dx--->(m,n)<--(m,k)(k,n)

注意：深度学习中，我们的目的不是计算微分矩阵，而是批量中每个样本单独计算的偏导数之和。
