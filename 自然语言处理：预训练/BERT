NLP里的迁移学习：
  1.使用预训练好的模型来抽取词、句子的特征
    例如word2vec或语言模型
  2.不更新训练好的模型
  3.需要构建新的网络来抓取新任务需要的信息
      Word2vec忽略了时序信息；语言模型只看一个方向

BERT的动机：
  1.基于微调的NLP模型
  2.预训练的模型抽取了足够多的信息
  3.新的任务只需要增加一个简单的输出层

  只有编码器的Transformer
  两个版本：
    Base：#blocks=12， hidden size=768, #head=12,
    #parameters=110M
    Large:#blocks=24, hidden size=1024, #heads=
    #parameters=340M

  在大规模数据上训练>3B词


  创新：
    1.对输入的修改：
      每个样本是一个句子对
      加入额外的片段嵌入(用来帮助区分两个句子)
      位置编码可学习
    2.预训练任务1：带掩码的语言模型
        --Transformer的编码器是双向的，标准语言模型要求单项
        --带掩码的语言模型每次随机将一些词元换成<mask>
        --因为微调任务中不出现<mask>（防止出现看到mask就预测的情况）
          --80%概率下，将选中的词元变成<mask>
          --10%的概率下换成一个随机词元
          --10%的概率下保持原有词元
    3.预训练任务2：下一个句子预测
      --预测一个句子对中两个句子是不是相邻
      --训练样本中：
        --50%的概率选择相邻句子对：<cls>this movie is great <sep> i like it <sep>
        --50%的概率选择随机句子对: <cls>this movie is great <sep> hello world <sep>
      --将<cls>对应的输出放到一个全连接层来预测

    BERT针对微调设计
    基于Transformer的编码器作了如下修改：
      --模型更大，训练数据更多
      --输入句子对；片段嵌入；可学习的位置编码
      --训练时使用两个任务：
        带掩码的语言模型
        下一个句子预测
    
    
