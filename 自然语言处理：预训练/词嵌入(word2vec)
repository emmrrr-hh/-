首先，
为什么独热编码不是一个好选择？
  无法表示不同词之间的相似度。比如经常使用的余弦相似度，因为不同编码之间的余弦为0，这就表示不了不同词元之间的相似度

为了解决上述问题，提出了word2vec工具，这个工具包含两个模型，分别是跳元模型和连续词袋

跳元模型：
    跳元模型假设一个词可以用来在文本序列中生成其周围的单词。
    采用的是极大似然估计。

连续词袋模型：
  连续词袋模型假设中心词是基于其在文本序列中的周围上下文词生成的。



近似训练：
  负采样：
    背景：
      在跳元模型和连续词袋的梯度计算中都包含求和。由于一个词表的数量可能较大，因此求梯度的开销也十分大。因此就相应提出了一些近似训练的方法，负采样和分层softmax
    基本思想：
    首先提出了一个计算P(ω(t+j)|ωt)的近似方法，sigmoid函数。
    另外，在跳元模型中计算P(ω(t+j)|ωt)采用的是考虑所有词元的softmax分布，要和词表V中所有的词元都计算一遍。
    而在负采样中，仅考虑w(t+j)作为ω(t)的上下文窗口和其他K个词不来自ω(t)的上下文窗口同时发生的概率，假设这(N+1)个事件是互相独立的。
    这样对于每一个P(ω(t+j)|ωt)的梯度计算成本就只和超参数K成线性关系

  层序softmax:
    这种构造方法可以使得基于任意词ωc生成词表V中的所有词的概率之和为1。（因为σ(x)=σ(-x)）
    根据这棵树就可以计算基于词ωc生成对应词的条件概率，给定不同的ωc，虽然对于同一个叶子节点的路径相同，但是ωc的向量不同，因此输出的结果也不同。


  预训练word2vec模型：
  采用一个近似的概率表达方式(用到了向量的点积和sigmoid函数)，损失函数采用的是二元交叉熵，label作为标签，1代表属于上下文，0代表不属于上下文。
同时采用mask作为掩码来消除填充字符的影响。mask中0代表填充字符，1代表非填充字符
  
